
// stopwords
nltk.download('stopwords') 


// spaCy Lemmatization
pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz 
pip install spacy 
!python3 -m spacy download en_core_web_sm 



try 1:
  text cleaning:
    text = re.sub(r"http\S+", "", text)
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = re.sub('\d+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    text = ' '.join(word for word in text.split() if len(word) > 3) # remove stopwors from text 
  parameters:
    MAX_NB_WORDS = 50000
    MAX_SEQUENCE_LENGTH = 250
    EMBEDDING_DIM = 100
    Epochs = 3
    Batchsize = 64
  results:
    Found 827822 unique tokens.
    Loss: 0.378
    Accuracy: 0.530

----------------------------------------------------------------------------------------------------

try 2 :
 Nimber of records = 100000
 text cleaning:
  1- links_email_remover
  2- clean_text
  3- lemmatize
  4- stopwords_remover
  5- number_removedr
  
 parameters:
  1- MAX_NB_WORDS = 30000
  2- MAX_SEQUENCE_LENGTH = 500
  3- EMBEDDING_DIM = 100
  4- Epochs = 3
  5- Batchsize = 256
 
  
 results:
 
 Befor applied lemma on the text 
 1- Found 320643 unique tokens befor applied lemma
  Acuurcy on test data set : 

Loss: 0.357
  Accuracy: 0.575

 
  Acuurcy on trainig data set : 
Epoch 1/3
317/317 [==============================] - 549s 2s/step - loss: 0.4071 - accuracy: 0.5635 - val_loss: 0.3598 - val_accuracy: 0.5821
Epoch 2/3
317/317 [==============================] - 503s 2s/step - loss: 0.3499 - accuracy: 0.5893 - val_loss: 0.3528 - val_accuracy: 0.5791
Epoch 3/3
317/317 [==============================] - 496s 2s/step - loss: 0.3252 - accuracy: 0.6282 - val_loss: 0.3570 - val_accuracy: 0.5816
 
 
 
 Found 308885 unique tokens after applied lemma and remove link without protocols
 Acuurcy on test data set : 
 
 *Loss: 0.3557
 *Accuracy: 0.5833
 
 Acuurcy on trainig data set : 
 Epoch 1/3
317/317 [==============================] - 494s 2s/step - loss: 0.4088 - accuracy: 0.5656 - val_loss: 0.3605 - val_accuracy: 0.5821
Epoch 2/3
317/317 [==============================] - 506s 2s/step - loss: 0.3536 - accuracy: 0.5845 - val_loss: 0.3559 - val_accuracy: 0.5836
Epoch 3/3
317/317 [==============================] - 493s 2s/step - loss: 0.3349 - accuracy: 0.6101 - val_loss: 0.3556 - val_accuracy: 0.5844


-----------------------------------------------------------------------------------------

try 3 : 
 
 
As try 3 except the batch size equal to 64 

Beffor lemma
 Acuurcy on trainig data set : 
Epoch 1/3
1266/1266 [==============================] - 798s 618ms/step - loss: 0.3776 - accuracy: 0.5777 - val_loss: 0.3532 - val_accuracy: 0.5829
Epoch 2/3
1266/1266 [==============================] - 812s 642ms/step - loss: 0.3366 - accuracy: 0.6092 - val_loss: 0.3510 - val_accuracy: 0.5780
Epoch 3/3
1266/1266 [==============================] - 804s 635ms/step - loss: 0.3002 - accuracy: 0.6623 - val_loss: 0.3654 - val_accuracy: 0.5677
 
 Acuurcy on test data set : 

   Loss: 0.365
  Accuracy: 0.570
 
***********************************************************************
  As try 3 except the batch size = 64 & MAX_SEQUENCE_LENGTH = 250
 
 Beffor lemma
 Acuurcy on trainig data set : 
 Epoch 1/3
1266/1266 [==============================] - 425s 333ms/step - loss: 0.3778 - accuracy: 0.5737 - val_loss: 0.3523 - val_accuracy: 0.5850
Epoch 2/3
1266/1266 [==============================] - 477s 376ms/step - loss: 0.3397 - accuracy: 0.6064 - val_loss: 0.3500 - val_accuracy: 0.5872
Epoch 3/3
1266/1266 [==============================] - 446s 352ms/step - loss: 0.3052 - accuracy: 0.6538 - val_loss: 0.3641 - val_accuracy: 0.5673
 
  Acuurcy on test data set : 

Loss: 0.364
  Accuracy: 0.567
-----------------------------------------------------------------------------


try 4 : 

 Nimber of records = 100000
 text cleaning:
  1- links_email_remover
  2- clean_text
  3- lemmatize
  4- stopwords_remover
  5- number_removedr
  
 parameters:
  1- MAX_NB_WORDS = 30000
  2- MAX_SEQUENCE_LENGTH = 500
  3- EMBEDDING_DIM = 100
  4- Epochs = 3
  5- Batchsize = 64
 
  
 results:
 TRainig data : 
 
 Epoch 1/3
1266/1266 [==============================] - 812s 640ms/step - loss: 0.3777 - accuracy: 0.5790 - val_loss: 0.3540 - val_accuracy: 0.5837
Epoch 2/3
1266/1266 [==============================] - 841s 664ms/step - loss: 0.3365 - accuracy: 0.6062 - val_loss: 0.3505 - val_accuracy: 0.5794
Epoch 3/3
1266/1266 [==============================] - 938s 741ms/step - loss: 0.3079 - accuracy: 0.6477 - val_loss: 0.3628 - val_accuracy: 0.5778


Test data : 
 Loss: 0.363
  Accuracy: 0.577


 -------------------------------------------------------------------
 
 try 5 : 

 Nimber of records = 370 000 
 text cleaning:
  1- links_email_remover
  2- clean_text
  3- lemmatize
  4- stopwords_remover
  5- number_removedr
  
 parameters:
  1- MAX_NB_WORDS = 500 000
  2- MAX_SEQUENCE_LENGTH = 500
  3- EMBEDDING_DIM = 100
  4- Epochs = 3
  5- Batchsize = 265
  
  result :
   1- Found 665338 unique tokens after applied lemma
   2- Found ** unique tokens befor applied lemma


  
 --------------------------------------------------------------------------------
 
 
 try 6 :
 Nimber of records = 348733

text sequence contains : 
 title + genra + description + content rating 
 
 text cleaning:
  1- links_email_remover
  2- clean_text
  3- lemmatize
  4- stemming
  5- stopwords_remover
  6- number_removedr
  7- remove word with numbers 
 
  
 parameters:
  1- MAX_NB_WORDS = 50000
  2- MAX_SEQUENCE_LENGTH = 500
  3- EMBEDDING_DIM = 100
  4- Epochs = 3
  5- Batchsize = 256
 
  
 results:
   
   On trainig data set : 
   
   Epoch 1/3
1104/1104 [==============================] - 1050s 949ms/step - loss: 1.1299 - accuracy: 0.5264 - val_loss: 1.0744 - val_accuracy: 0.5468
Epoch 2/3
1104/1104 [==============================] - 1064s 964ms/step - loss: 1.0293 - accuracy: 0.5748 - val_loss: 1.0751 - val_accuracy: 0.5471
Epoch 3/3
1104/1104 [==============================] - 1030s 933ms/step - loss: 0.9680 - accuracy: 0.6069 - val_loss: 1.0950 - val_accuracy: 0.5437 
9809/9809 [==============================] - 865s 88ms/step - loss: 0.9180 - accuracy: 0.6338


On Test set
Loss: 0.918
Accuracy: 0.634 

