
// stopwords
nltk.download('stopwords') 


// spaCy Lemmatization
pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz 
pip install spacy 
!python3 -m spacy download en_core_web_sm 



try 1:
  text cleaning:
    text = re.sub(r"http\S+", "", text)
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = re.sub('\d+', '', text)
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    text = ' '.join(word for word in text.split() if len(word) > 3) # remove stopwors from text 
  parameters:
    MAX_NB_WORDS = 50000
    MAX_SEQUENCE_LENGTH = 250
    EMBEDDING_DIM = 100
    Epochs = 3
    Batchsize = 64
  results:
    Found 827822 unique tokens.
    Loss: 0.378
    Accuracy: 0.530

----------------------------------------------------------------------------------------------------

try 2 :
 Nimber of records = 100000
 text cleaning:
  1- links_email_remover
  2- clean_text
  3- lemmatize
  4- stopwords_remover
  5- number_removedr
  
 parameters:
  1- MAX_NB_WORDS = 30000
  2- MAX_SEQUENCE_LENGTH = 500
  3- EMBEDDING_DIM = 100
  4- Epochs = 3
  5- Batchsize = 256
 
  
 results:
 
 Befor applied lemma on the text 
 1- Found 320643 unique tokens befor applied lemma
  Acuurcy on test data set : 

Loss: 0.357
  Accuracy: 0.575

 
  Acuurcy on trainig data set : 
Epoch 1/3
317/317 [==============================] - 549s 2s/step - loss: 0.4071 - accuracy: 0.5635 - val_loss: 0.3598 - val_accuracy: 0.5821
Epoch 2/3
317/317 [==============================] - 503s 2s/step - loss: 0.3499 - accuracy: 0.5893 - val_loss: 0.3528 - val_accuracy: 0.5791
Epoch 3/3
317/317 [==============================] - 496s 2s/step - loss: 0.3252 - accuracy: 0.6282 - val_loss: 0.3570 - val_accuracy: 0.5816
 
 
 
 Found 308885 unique tokens after applied lemma and remove link without protocols
 Acuurcy on test data set : 
 
 *Loss: 0.3557
 *Accuracy: 0.5833
 
 Acuurcy on trainig data set : 
 Epoch 1/3
317/317 [==============================] - 494s 2s/step - loss: 0.4088 - accuracy: 0.5656 - val_loss: 0.3605 - val_accuracy: 0.5821
Epoch 2/3
317/317 [==============================] - 506s 2s/step - loss: 0.3536 - accuracy: 0.5845 - val_loss: 0.3559 - val_accuracy: 0.5836
Epoch 3/3
317/317 [==============================] - 493s 2s/step - loss: 0.3349 - accuracy: 0.6101 - val_loss: 0.3556 - val_accuracy: 0.5844


-----------------------------------------------------------------------------------------

try 3 : 
 
 
As try 3 except the batch size equal to 64 
 Acuurcy on trainig data set : 

 
 Acuurcy on test data set : 

  
 

  
  
